[EbmModel]
layer_widths = 40, 81
spline_degree = 3
layernorm = false
base_activation = leakyrelu
spline_function = FFT
grid_size = 50
grid_update_ratio = 0.05
grid_range = -1,1
ε_scale = 1
μ_scale = 1
σ_base = 1
σ_spline = 1
init_τ = 1
τ_trainable = true
π_0 = ebm
GaussQuad_nodes = 200
quadrature_method = gausslegendre

[MixtureModel]
use_mixture_prior = true
λ_reg = 0.0
use_attention_kernel = false
train_proportions = false

[GeneratorModel]
widths = 81, 162
spline_degree = 3
layernorm = false
base_activation = gelu
spline_function = FFT
grid_size = 100
grid_update_ratio = 0.05
grid_range = -1.5,1.5 
ε_scale = 1
μ_scale = 1
σ_base = 1
σ_spline = 1
init_τ = 1
τ_trainable = true
generator_variance = 0.3
generator_noise = 0.01
output_activation = sigmoid
resampler = residual
perceptual_scale = 0.0001

[GRID_UPDATING]
update_prior_grid = true
update_llhood_grid = false
grid_update_frequency = 50
grid_update_decay = 0.9999

[PRIOR_LANGEVIN]
step_size = 0.16
iters = 40

[POST_LANGEVIN]
use_langevin = true
initial_step_size = 0.001
step_size_bounds = 0.00001, 10
iters = 40

[THERMODYNAMIC_INTEGRATION]
num_temps = 10
p_start = 0.8
p_end = 4
num_cycles = 0
N_langevin_per_temp = 40

[TRAINING]
batch_size = 50
N_train = 20000
N_test = 100
num_generated_samples = 3000
N_epochs = 20
use_gpu = true
dataset = MNIST
update_grid = true
verbose = true
contrastive_divergence_training = true
resampling_threshold_factor = 0.5
eps = 0.0001
checkpoint_every = 5
gen_every = 2
use_perceptual_loss = false

[SEQ]
sequence_length = -1
vocab_size = 1000
activation = relu
d_model = 128

[CNN]
use_cnn_lkhood = true
hidden_feature_dims = 512, 256, 128, 64
strides = 1, 2, 2, 2, 2
kernel_sizes = 4, 4, 4, 4, 4
paddings = 0, 1, 1, 1, 1
activation = leakyrelu
batchnorm = false
latent_concat = false

[OPTIMIZER]
type = nesterov
learning_rate = 0.001
betas = 0.9,0.999
decay = 0
ρ = 0.9
ε = 0.000000008

[PCA]
use_pca = false
pca_components = 800
